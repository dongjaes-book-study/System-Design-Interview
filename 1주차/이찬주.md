# 의문점
1. 어느정도 트래픽을 기준으로 로드밸런서를 도입할 것인지를 고민할 생각이 들지
2. 데이터베이스의 크기는 어느정도면 확장에 대해서 고려해야할 지 

# 1장 사용자 수에 따른 규모 확장성

사용자 수에 비례해서 아키텍처를 어떻게 발전해나갈 것인가?

수직적? 수평적?

NoSQL, SQL 중 선택, 로드밸런서, 데이터베이스 다중화, 캐시

## 캐시
어떤 상황에서 특정 데이터에 적용하는 가?
보관된 데이터의 만료 기간은?
데이터 베이스와의 일관성과 장애시 대처 방법은?

## CDN
돈 얼마나 드는지 -> 모든 데이터를 다 CDN에 저장할 필요는 없다
보관된 데이터의 만료 기간은?
CDN에 장애가 났을 때 대처 방법은?

## stateless

stateful 하면 각 서버로 요청을 보내야됨
-> 하나의 공유 저장소를 두고 웹 서버에서는 이를 기반으로 요청 처리
-> 트래픽이 증가할 것을 우려해 로드밸런서 + 캐시 등 도입, NoSQL를 쓰는 이유 중 하나는 확장하기 편해서
-> 전세계적으로 쾌적하게 쓸 수 있도록 여러 데이터 센터를 지원, 로드밸런서를 기반으로 서로 다른 데이터 센터를 찌를 수 있도록

## 넷플릭스에서 statless하게 서버 배포하는 방법

https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b


 >failure of any kind in one Region should not affect services running in another, a networking partitioning event should not affect quality of service in either Region.
 즉 다른 하나의 리전에서 생긴 문제가 다른 리전에게 연향을 주면 안됨

이와 같은 목표를 가지고 넷플릭스에서는 Active - Active의 형태로 배포를 진행

즉 stateless하게 똑같은 아키텍처를 여러 지역에 만들어서 관리

아래와 같은 기술적 과제가 생김

> 1. Effective tooling for directing traffic to the correct Region
> 2. Traffic shaping and load shedding, to handle thundering herd events
> 3. State / Data asynchronous cross-regional replication
- 지역마다 만든 시스템들끼리의 데이터베이스 일관성 유지, 로드밸런서를 통한 효율적인 트래픽 분배가 필요

이에 대해서 DNS(분모로 사용자 트래픽 제어, UltraDNS), Zuul(트래픽 형성), Cassandra 및 EvCache(데이터 복제)를 통해서 해결

위와 같은 다중 지역 배포 프로세스를 더욱 자동화하기 위해서,  Glisten 워크플로 언어를 기반으로 하는 Mimir라는 워크플로 도구를 개발
-> 이를 자동화된 카나리아 분석 및 자동화된 롤백 절차와 결합하면 애플리케이션을 단계별 작업 순서로 여러 위치에 자동으로 배포 

테스트는 유인원 집합이 있다고 생각하고 진행 -> 이게 카오스 몽키인 듯

## 메시지 큐
메시지의 무손실(메시지 큐에 일단 보관된 메시지는 소비자가 꺼낼 때 까지 안전하게 보관)인 특성이 있음 -> 각 컴포넌트들끼리 Loosly coupled

특정 이벤트를 저장하다가 원하면 던져주는거임
받은 입장에서는 해당 이벤트를 처리해주는 거

## 수직적, 수평적 확장
데이터베이스에 용량이 많아짐 -> 데이터베이스가 견뎌야할 부하 증가 어케 처리?

### 수직적
스케일 업
1. 하드웨어의 한계가 있어서 무한정 증설 불가
2. SPOF로 인한 위험성 큼
3. 고성능 서버로 갈수록 가격이 높아짐

### 수평적
대규모 데이터베이스를 샤드라는 단위로 나눠서 저장
ID를 %와 같은 연산을 통해 나눠서 저장
1. 데이터의 재샤딩
2. 유명인사 문제 (핫스팟)
3. 조인과 비정규화 (여러 샤드간 조인하기 힘듦)

# 2장 개략적인 규모 추정

## 개략적 규모 추정
ASCII 문자 하나는 1바이트

- 메모리는 빠르지만 디스크는 아직도 느림
- 디스크 탐색(seek)은 최대한 피하기
- 단순한 압축 알고리즘은 빠름
- 데이터를 인터넷으로 전송하기 전에 가능하면 압축
- 데이터 센터는 보통 여러 지역(region)에 분산되어 있고, 센터들 간에 데이터를 주고 받는 데 시간이 걸림

## 트위터 QPS와 저장소 계산
### 가정
MAU = 3억명
사용자 중 50% 트위터 사용
평균적으로 각 사용자는 매일 2건 트윗 올림
미디어 포함 트윗 10%
데이터는 5년간 보관

### 추정
QPS(Query Per Second) 추정치
DAU = 3억 * 50% = 1.5억
QPS = 1.5억 * 2 트윗 / 24시간 / 3600초 = 약 3500
최대 QPS(Peek QPS) = 2 * QPS = 약 7000

## 팁
99987/9.1 = 1000000 / 10 로 근사치를 말할 수 있음


# 3장 시스템 설계 면접 공략법
설계의 순수성에 집착한 나머지 타협적 결정을 도외시하고 과도한 엔지니어링을 하는 경우가 많음

그러지 말고 더 깊이 생각 ㄱ

## 1. 문제 이해 및 설계 범위 확장
- 구체적으로 어떤 기능을 만들어야 됨?
- 제품 사용자 수는 얼마나 됨?
- 회사의 규모는 얼마나 빨리 커지리라 예상? 3달, 6달, 1년 단위로 대답좀
- 회사가 주로 사용하는 기술 스택은? 설계를 단순화할 수 있는 서비스는 뭐가 있는 가

## 2. 개략적인 설계안 제시 및 동의 구하기

면접자를 팀원이라 생각하고 아키텍처 그리셈

내부적인 api나 그런 것들은 규모봐서 질문 ㄱ

## 3. 상세 설계
불필요한 세부사항에 시간 쓰지마셈

## 4. 마무리
병목 지점이나 개선 가능한 지점을 말하라고 할 수 있음

설계한 요소들에 대해서 요약해보셈

로그는? 배포 전략은?


# 4장 처리율 제한장치의 설계

처리율 제한 장치는 정해진 임계치를 넘어서면, 추가로 도달한 모든 호출은 처리가 중단(block)

1. DoS 막기 위해서 제한
2. 비용이 많이 드는 API 호출에 대한 제한
3. 서버 과부하를 막기 위해서

## 1. 요구사항 파악
어떤 layer에서 필요한 것 인지

어느정도 규모에서 필요한 것 인지

예외처리 어케할건지

얼만큼 버텨내야하는 지

## 2. 개략적 설계안 제시 및 설계 범위 확장

HTTP 429 -> 사용자가 너무 많은 요청을 보냄
보통 처리율 제한 장치는 보통 API 게이트웨이를 통해서 구현함
- 처리율 제한
- SSL 종단
- 사용자 인증
### 토큰 버킷 알고리즘
토큰을 버킷에 정해진 양만큼 넣어두고, 하나의 요청당 하나의 토큰을 사용함

토큰 버킷의 크기는 토큰이 들어있을 수 있는 것이고, 토큰 공급률은 분당 4라는 것은 분마다 4개 만들어진다는 거임

### 누출 버킷 알고리즘
위의 알고리즘에 큐를 추가했다고 보면 됨

### 고정 윈도 카운터 알고리즘
시간마다 윈도우를 열고 해당 윈도우에서 받을 수 있는 요청의 한계점을 설정, 해당 한계점을 넘어서면 다음 윈도우가 열릴 때 까지 기다리기

### 이동 윈도 로깅 알고리즘
고정 윈도우 알고리즘은 윈도우 경계 부근에 트래픽이 몰리면, 시스템에서 설정한 한도보다 많은 요청을 처리하게 됨

이걸 개선하기 위해서 만들어진 알고리즘
등 많은 알고리즘이 있음

## 3. 상세 설계

어떤 요청이 한도 제한에 걸리면 API는 HTTP 429 응답을 클라이언트에서 보냄

특정 HTTP 헤더를 보냄
처리율 제한 미들웨어는 제한 규칙을 캐시에서 가져옴
카운터 및 마지막 요청의 타임스탬프를 레디스 캐시에서 가져옴

처리율 제한을 구현하려면 counter를 구현해야됨 ->
여러 대 서버랑 병렬 스레드를 지원하도록 시스템을 확장하면 다음과 같은 것들을 고려해야됨
- 경쟁 조건 (락)
- 동기화 

특정 클라이언트에 대해서 처리율 제한을 하려면, 똑같은 서버에게 계속 요청을 보내야됨 -> sticky session

근데 별로 확장성이 좋지 않으므로, 레디스와 같은 중앙 집중식 데이터베이스를 사용하는 게 좋음

## 4. 마무리

처리율 제한 구현하는 알고리즘
- 토큰 버킷
- 누출 버킷
- 고정 윈도 카운터
- 이동 윈도 로그
- 이동 윈도 카운터






